{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCP Tools Evaluation with flex-evals\n",
    "\n",
    "This notebook demonstrates how to evaluate LLM tool-calling capabilities using the [flex-evals](https://github.com/shane-kercheval/flex-evals) framework.\n",
    "\n",
    "We'll verify that `gpt-4.1-mini` correctly predicts tool arguments for our MCP tools (e.g., `edit_content`), then execute those predictions and verify the actual results.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, ensure your `.env` file has:\n",
    "- `VITE_DEV_MODE=true` (bypasses auth)\n",
    "- `OPENAI_API_KEY=sk-...`\n",
    "\n",
    "1. **Start Docker containers** (PostgreSQL + Redis):\n",
    "   ```bash\n",
    "   make docker-up\n",
    "   ```\n",
    "\n",
    "2. **Run database migrations** (if needed):\n",
    "   ```bash\n",
    "   make migrate\n",
    "   ```\n",
    "\n",
    "3. **Start the API server** (port 8000):\n",
    "   ```bash\n",
    "   make run\n",
    "   ```\n",
    "\n",
    "4. **Start the Content MCP server** (port 8001) - in a separate terminal:\n",
    "   ```bash\n",
    "   make content-mcp-server\n",
    "   ```\n",
    "\n",
    "5. **Node.js** (for `npx` to run `mcp-remote`):\n",
    "   - Ensure Node.js is installed\n",
    "   - `npx` will automatically download `mcp-remote` on first use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Dummy token - works with VITE_DEV_MODE=true\n",
    "PAT_TOKEN = \"bm_devtoken\"\n",
    "\n",
    "# MCP Server URL\n",
    "CONTENT_MCP_URL = \"http://localhost:8001/mcp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sik_llms.mcp_manager import MCPClientManager\n",
    "\n",
    "mcp_config = {\n",
    "    \"mcpServers\": {\n",
    "        \"content\": {\n",
    "            \"command\": \"npx\",\n",
    "            \"args\": [\n",
    "                \"mcp-remote\",\n",
    "                CONTENT_MCP_URL,\n",
    "                \"--header\",\n",
    "                f\"Authorization: Bearer {PAT_TOKEN}\",\n",
    "            ],\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-End Eval: Tool Prediction + Execution\n",
    "\n",
    "This eval:\n",
    "1. Creates a note with a typo\n",
    "2. Uses a realistic prompt (no hardcoded answer)\n",
    "3. Gets the LLM's tool prediction\n",
    "4. Executes the tool\n",
    "5. Verifies both the prediction and final content\n",
    "\n",
    "The test cases define expected values, and checks reference them via JSONPath - making it scalable to multiple test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test case data - each case has content with a typo and expected outcomes\n",
    "TEST_CASES_DATA = [\n",
    "    {\n",
    "        \"content\": \"This docu needs to be updated.\\n\\nSee the documentation for more details.\",\n",
    "        \"typo\": \"docu\",  # What to search for\n",
    "        \"expected\": {\n",
    "            \"tool_name\": \"edit_content\",\n",
    "            \"old_str_contains\": \"docu\",\n",
    "            \"new_str_contains\": \"document\",\n",
    "            \"final_must_contain\": \"document\",\n",
    "            \"final_must_not_contain\": \"documentmentation\",  # Corruption check\n",
    "        },\n",
    "        \"description\": \"Fix 'docu' -> 'document' without corrupting 'documentation'\",\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"The recieve function handles incoming data.\\n\\nUsers will receive a confirmation email.\",  # noqa: E501\n",
    "        \"typo\": \"recieve\",  # Common misspelling\n",
    "        \"expected\": {\n",
    "            \"tool_name\": \"edit_content\",\n",
    "            \"old_str_contains\": \"recieve\",\n",
    "            \"new_str_contains\": \"receive\",\n",
    "            \"final_must_contain\": \"receive function\",\n",
    "            \"final_must_not_contain\": \"receivee\",  # Corruption check\n",
    "        },\n",
    "        \"description\": \"Fix 'recieve' -> 'receive' without corrupting second 'receive'\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "from sik_llms import create_client, RegisteredClients, user_message\n",
    "from flex_evals import TestCase, Output, ExactMatchCheck, ContainsCheck, evaluate\n",
    "\n",
    "\n",
    "async def get_tool_prediction(prompt: str, tools: list) -> dict:\n",
    "    \"\"\"Get the LLM's tool prediction for a given prompt.\"\"\"\n",
    "    client = create_client(\n",
    "        client_type=RegisteredClients.OPENAI_TOOLS,\n",
    "        model_name=\"gpt-4.1-mini\",\n",
    "        tools=tools,\n",
    "    )\n",
    "    response = await client.run_async(messages=[user_message(prompt)])\n",
    "\n",
    "    if response.tool_prediction:\n",
    "        return {\n",
    "            \"tool_name\": response.tool_prediction.name,\n",
    "            \"arguments\": response.tool_prediction.arguments,\n",
    "        }\n",
    "    return {\"tool_name\": None, \"arguments\": {}}\n",
    "\n",
    "\n",
    "async def run_eval_case(mcp, case_data: dict, tools: list) -> dict:  # noqa: ANN001\n",
    "    \"\"\"\n",
    "    Run a single eval case end-to-end.\n",
    "\n",
    "    1. Create note with content\n",
    "    2. Search for the typo\n",
    "    3. Get tool prediction with realistic prompt\n",
    "    4. Execute the tool\n",
    "    5. Get final content\n",
    "    6. Delete the note\n",
    "\n",
    "    Returns dict with all data for evaluation.\n",
    "    \"\"\"\n",
    "    # Create the note\n",
    "    create_result = await mcp.call_tool(\"create_note\", {\n",
    "        \"title\": \"Eval Test Note\",\n",
    "        \"content\": case_data[\"content\"],\n",
    "        \"tags\": [\"eval-test\"],\n",
    "    })\n",
    "    note_data = json.loads(create_result.content[0].text)\n",
    "    note_id = note_data[\"id\"]\n",
    "\n",
    "    try:\n",
    "        # Search for the typo (like an agent would)\n",
    "        search_result = await mcp.call_tool(\"search_in_content\", {\n",
    "            \"id\": note_id,\n",
    "            \"type\": \"note\",\n",
    "            \"query\": case_data[\"typo\"],\n",
    "        })\n",
    "        search_response = json.dumps(search_result.structuredContent, indent=2)\n",
    "\n",
    "        # Build a realistic prompt - agent knows there's a typo but doesn't know the fix\n",
    "        prompt = f\"\"\"I found a typo in this note. Please fix it.\n",
    "\n",
    "Note ID: {note_id}\n",
    "Type: note\n",
    "\n",
    "search_in_content result for \"{case_data[\"typo\"]}\":\n",
    "```json\n",
    "{search_response}\n",
    "```\n",
    "\n",
    "Fix the typo.\"\"\"\n",
    "\n",
    "        # Get tool prediction\n",
    "        prediction = await get_tool_prediction(prompt, tools)\n",
    "\n",
    "        # Execute the tool if it's edit_content\n",
    "        tool_result = None\n",
    "        final_content = None\n",
    "\n",
    "        if prediction[\"tool_name\"] == \"edit_content\":\n",
    "            edit_result = await mcp.call_tool(\"edit_content\", prediction[\"arguments\"])\n",
    "            if not edit_result.isError:\n",
    "                tool_result = edit_result.structuredContent\n",
    "\n",
    "                # Get final content\n",
    "                get_result = await mcp.call_tool(\"get_content\", {\"id\": note_id, \"type\": \"note\"})\n",
    "                if not get_result.isError:\n",
    "                    final_content = get_result.structuredContent.get(\"content\")\n",
    "\n",
    "        return {\n",
    "            \"note_id\": note_id,\n",
    "            \"prompt\": prompt,\n",
    "            \"tool_prediction\": prediction,\n",
    "            \"tool_result\": tool_result,\n",
    "            \"final_content\": final_content,\n",
    "        }\n",
    "    finally:\n",
    "        # Clean up: delete the note\n",
    "        async with httpx.AsyncClient() as client:\n",
    "            await client.delete(\n",
    "                f\"http://localhost:8000/notes/{note_id}?permanent=true\",\n",
    "                headers={\"Authorization\": f\"Bearer {PAT_TOKEN}\"},\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example eval case result:\n",
      "{\n",
      "  \"note_id\": \"019bdea3-ae64-7ef4-a85c-de5bc1ee2b8d\",\n",
      "  \"prompt\": \"I found a typo in this note. Please fix it.\\n\\nNote ID: 019bdea3-ae64-7ef4-a85c-de5bc1ee2b8d\\nType: note\\n\\nsearch_in_content result for \\\"docu\\\":\\n```json\\n{\\n  \\\"matches\\\": [\\n    {\\n      \\\"field\\\": \\\"content\\\",\\n      \\\"line\\\": 1,\\n      \\\"context\\\": \\\"This docu needs to be updated.\\\\n\\\\nSee the documentation for more details.\\\"\\n    },\\n    {\\n      \\\"field\\\": \\\"content\\\",\\n      \\\"line\\\": 3,\\n      \\\"context\\\": \\\"This docu needs to be updated.\\\\n\\\\nSee the documentation for more details.\\\"\\n    }\\n  ],\\n  \\\"total_matches\\\": 2\\n}\\n```\\n\\nFix the typo.\",\n",
      "  \"tool_prediction\": {\n",
      "    \"tool_name\": \"edit_content\",\n",
      "    \"arguments\": {\n",
      "      \"id\": \"019bdea3-ae64-7ef4-a85c-de5bc1ee2b8d\",\n",
      "      \"type\": \"note\",\n",
      "      \"old_str\": \"This docu needs to be updated.\",\n",
      "      \"new_str\": \"This document needs to be updated.\"\n",
      "    }\n",
      "  },\n",
      "  \"tool_result\": {\n",
      "    \"response_type\": \"minimal\",\n",
      "    \"match_type\": \"exact\",\n",
      "    \"line\": 1,\n",
      "    \"data\": {\n",
      "      \"id\": \"019bdea3-ae64-7ef4-a85c-de5bc1ee2b8d\",\n",
      "      \"updated_at\": \"2026-01-21T03:40:29.370968Z\"\n",
      "    }\n",
      "  },\n",
      "  \"final_content\": \"This document needs to be updated.\\n\\nSee the documentation for more details.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Example: Run a single eval case to see the output structure\n",
    "async with MCPClientManager(mcp_config) as mcp:\n",
    "    tools = mcp.get_tools()\n",
    "    example_result = await run_eval_case(mcp, TEST_CASES_DATA[0], tools)\n",
    "\n",
    "print(\"Example eval case result:\")\n",
    "print(json.dumps(example_result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Eval Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: Fix 'docu' -> 'document' without corrupting 'documentation'\n",
      "  Tool: edit_content\n",
      "  Args: {\n",
      "    \"id\": \"019bdea3-b987-71c8-9199-611a7616e364\",\n",
      "    \"type\": \"note\",\n",
      "    \"old_str\": \"This docu needs to be updated.\",\n",
      "    \"new_str\": \"This document needs to be updated.\"\n",
      "}\n",
      "  Final: 'This document needs to be updated.\\n\\nSee the documentation for more details.'\n",
      "\n",
      "Running: Fix 'recieve' -> 'receive' without corrupting second 'receive'\n",
      "  Tool: edit_content\n",
      "  Args: {\n",
      "    \"id\": \"019bdea3-c0e4-772e-9090-fa764b94a30a\",\n",
      "    \"type\": \"note\",\n",
      "    \"old_str\": \"The recieve function handles incoming data.\",\n",
      "    \"new_str\": \"The receive function handles incoming data.\"\n",
      "}\n",
      "  Final: 'The receive function handles incoming data.\\n\\nUsers will receive a confirmation email.'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run all eval cases and collect results\n",
    "eval_results = []\n",
    "\n",
    "async with MCPClientManager(mcp_config) as mcp:\n",
    "    tools = mcp.get_tools()\n",
    "\n",
    "    for case_data in TEST_CASES_DATA:\n",
    "        print(f\"Running: {case_data['description']}\")\n",
    "        result = await run_eval_case(mcp, case_data, tools)\n",
    "        eval_results.append(result)\n",
    "\n",
    "        print(f\"  Tool: {result['tool_prediction']['tool_name']}\")\n",
    "        print(f\"  Args: {json.dumps(result['tool_prediction']['arguments'], indent=4)}\")\n",
    "        print(f\"  Final: {result['final_content']!r}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "  Status: completed\n",
      "  Total: 2, Completed: 2, Errors: 0\n",
      "\n",
      "Fix 'docu' -> 'document' without corrupting 'documentation':\n",
      "  [PASS] exact_match\n",
      "  [PASS] contains\n",
      "  [PASS] contains\n",
      "  [PASS] contains\n",
      "\n",
      "Fix 'recieve' -> 'receive' without corrupting second 'receive':\n",
      "  [PASS] exact_match\n",
      "  [PASS] contains\n",
      "  [PASS] contains\n",
      "  [PASS] contains\n"
     ]
    }
   ],
   "source": [
    "# Build TestCases and Outputs from our results\n",
    "test_cases = []\n",
    "outputs = []\n",
    "\n",
    "for case_data, result in zip(TEST_CASES_DATA, eval_results):\n",
    "    test_cases.append(TestCase(\n",
    "        input=result[\"prompt\"],\n",
    "        expected=case_data[\"expected\"],\n",
    "        metadata={\"description\": case_data[\"description\"]},\n",
    "    ))\n",
    "    outputs.append(Output(\n",
    "        value=result,\n",
    "        metadata={\"model\": \"gpt-4.1-mini\"},\n",
    "    ))\n",
    "\n",
    "# Define checks that reference expected values via JSONPath\n",
    "# Note: phrases takes a single JSONPath string (not a list) - it gets converted to a list\n",
    "# internally\n",
    "checks = [\n",
    "    # Verify correct tool was selected\n",
    "    ExactMatchCheck(\n",
    "        actual=\"$.output.value.tool_prediction.tool_name\",\n",
    "        expected=\"$.test_case.expected.tool_name\",\n",
    "    ),\n",
    "    # Verify old_str contains what we expect\n",
    "    ContainsCheck(\n",
    "        text=\"$.output.value.tool_prediction.arguments.old_str\",\n",
    "        phrases=\"$.test_case.expected.old_str_contains\",\n",
    "    ),\n",
    "    # Verify new_str contains what we expect\n",
    "    ContainsCheck(\n",
    "        text=\"$.output.value.tool_prediction.arguments.new_str\",\n",
    "        phrases=\"$.test_case.expected.new_str_contains\",\n",
    "    ),\n",
    "    # Verify final content contains the fix\n",
    "    ContainsCheck(\n",
    "        text=\"$.output.value.final_content\",\n",
    "        phrases=\"$.test_case.expected.final_must_contain\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "results = await evaluate(test_cases, outputs, checks)\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "print(f\"  Status: {results.status}\")\n",
    "print(f\"  Total: {results.summary.total_test_cases}, Completed: {results.summary.completed_test_cases}, Errors: {results.summary.error_test_cases}\")  # noqa: E501\n",
    "\n",
    "for i, result in enumerate(results.results):\n",
    "    desc = test_cases[i].metadata.get(\"description\", f\"Case {i+1}\")\n",
    "    print(f\"\\n{desc}:\")\n",
    "    for check_result in result.check_results:\n",
    "        passed = check_result.results.get('passed', False)\n",
    "        status = \"PASS\" if passed else \"FAIL\"\n",
    "        print(f\"  [{status}] {check_result.check_type}\")\n",
    "        if not passed:\n",
    "            print(f\"         resolved_arguments: {json.dumps(check_result.resolved_arguments, indent=12)}\")  # noqa: E501\n",
    "            print(f\"         results: {check_result.results}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
