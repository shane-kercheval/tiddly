# UUIDv7 Primary Key Implementation Plan

## Overview

Replace sequential integer primary keys with UUIDv7 for all content tables. This eliminates information leakage from auto-increment IDs while providing time-sortable, globally unique identifiers.

### Problem

Current auto-increment integer IDs leak information:
- Competitors can infer total record counts ("my bookmark is ID 50,000")
- Growth rates can be estimated by creating accounts over time
- Sequential IDs provide enumeration surface for attackers

### Solution: UUIDv7 as Sole Primary Key

Replace integer PKs with UUIDv7:
- **Single ID**: One `id` column that is both the PK and API identifier
- **Time-sortable**: UUIDv7 embeds a Unix timestamp (millisecond precision)
- **Native PostgreSQL**: Uses `uuid` type (16 bytes binary, efficient indexing)
- **No information leakage**: Random component prevents inference
- **Distributed-ready**: IDs can be generated on any node without coordination (enables future sharding, multi-region, microservices)

### Why UUIDv7 Over Other Options

| Option | Storage | Time-sortable | Native PG type | Standard |
|--------|---------|---------------|----------------|----------|
| UUIDv4 | 16 bytes | No | Yes | RFC 4122 |
| UUIDv7 | 16 bytes | Yes | Yes | RFC 9562 |
| ULID | 26 bytes (string) | Yes | No | Informal |
| Dual ID | 4+26 bytes | N/A | N/A | N/A |

**UUIDv7 wins because:**
1. Native PostgreSQL `uuid` type = smaller storage, better indexing than string-based ULID
2. Time-sortable = sequential insert performance (unlike UUIDv4)
3. RFC 9562 standard = better interoperability
4. Simpler than dual-ID approach = less code, fewer migrations, no mapping logic

### Tables to Migrate

Based on API analysis, these tables expose integer IDs:

| Table | API Exposure | Foreign Keys |
|-------|--------------|--------------|
| `bookmarks` | `/bookmarks/{id}`, responses | `user_id`, junction tables |
| `notes` | `/notes/{id}`, responses | `user_id`, junction tables |
| `prompts` | `/prompts/{id}`, responses | `user_id`, junction tables |
| `content_lists` | `/lists/{id}`, responses, `list_id` query params | `user_id` |
| `tags` | Tag responses, rename endpoint | `user_id`, junction tables |
| `api_tokens` | `/tokens/{id}`, responses | `user_id` |
| `note_versions` | Internal (future use) | `note_id` FK to notes |

**Tables NOT migrated:**
- `users`: Internal-only ID (Auth0 users never see it; exposed ID is `auth0_id`)
- `user_consent`: Internal-only (consent endpoint returns consent data, not ID)
- `user_settings`: Internal-only (settings endpoint returns settings, not ID)
- Junction tables (`bookmark_tags`, etc.): Will be updated to reference new UUIDs

### Key Design Decisions

1. **UUID generation**: Generate in Python at model instantiation using `uuid7` library
2. **Field name unchanged**: API continues using `id` - only the type changes (`int` → `str`)
3. **URL format**: UUIDs with hyphens are URL-safe (e.g., `/bookmarks/01938a12-3b45-7c67-8d90-ef1234567890`)
4. **Minimal breaking change**: Only the type changes; clients using `/bookmarks/${bookmark.id}` continue working
5. **Single migration**: All tables migrated in one atomic transaction

### Documentation

Read before implementing:
- UUIDv7 RFC 9562: https://www.rfc-editor.org/rfc/rfc9562#name-uuid-version-7
- uuid7 Python library: https://pypi.org/project/uuid7/

---

## Milestone 1: Add UUID7 Library and Create ID Mixin

### Goal
Add UUIDv7 generation capability and create a mixin that replaces integer PKs.

### Success Criteria
- `uuid7` library installed
- `UUIDv7Mixin` created that defines `id` as UUID primary key
- UUID auto-generates on model instantiation
- Unit tests verify UUID generation and format

### Key Changes

**1. Add dependency:**

```bash
uv add uuid7
```

**2. Create `UUIDv7Mixin` in `backend/src/models/base.py`:**

```python
from uuid import UUID
from uuid7 import uuid7
from sqlalchemy.dialects.postgresql import UUID as PG_UUID

class UUIDv7Mixin:
    """Mixin that provides a UUIDv7 primary key."""

    id: Mapped[UUID] = mapped_column(
        PG_UUID(as_uuid=True),
        primary_key=True,
        default=uuid7,
    )
```

Notes:
- `uuid7()` returns a standard `uuid.UUID` object
- `as_uuid=True` means SQLAlchemy handles UUID↔string conversion automatically
- No need for `str(uuid7())` - the type is native

### Testing Strategy
- Test UUID is auto-generated when not provided
- Test UUID format is valid (version 7, correct structure)
- Test UUIDs are time-ordered (create two in sequence, confirm ordering)
- Test custom UUID can be provided (for testing/seeding)

### Dependencies
None - this is the foundation.

### Risk Factors
- Verify `default=uuid7` works correctly with SQLAlchemy (function reference, not call)

---

## Milestone 2: Database Migration (All Tables)

### Goal
Single Alembic migration that converts all 7 tables from integer PKs to UUIDv7 PKs, including updating all 3 junction table foreign keys.

### Success Criteria
- All tables converted: `bookmarks`, `notes`, `prompts`, `content_lists`, `tags`, `api_tokens`, `note_versions`
- All junction tables updated: `bookmark_tags`, `note_tags`, `prompt_tags`
- `note_versions.note_id` FK updated to reference new `notes.id` UUID
- Migration is atomic (all or nothing)
- Migration tested against production backup
- Rollback documented (restore from backup)

### Key Changes

**Create single Alembic migration:**

```bash
make migration message="convert all content tables to uuid7 primary keys"
```

**Migration script structure:**

```python
"""Convert all content tables to UUID7 primary keys.

Tables converted:
- bookmarks (+ bookmark_tags junction)
- notes (+ note_tags junction + note_versions FK)
- prompts (+ prompt_tags junction)
- content_lists (+ sidebar_order JSONB update)
- tags
- api_tokens
- note_versions

Cleanup:
- Drop deprecated tab_order column from user_settings

Revision ID: xxxx
"""
from uuid7 import uuid7
from sqlalchemy.dialects.postgresql import UUID as PG_UUID
import sqlalchemy as sa
from alembic import op

# Tables with junction table dependencies
# Format: {table: {'junction': junction_table, 'dependents': [(table, fk_col), ...]}}
TABLES_WITH_JUNCTIONS = {
    'bookmarks': {
        'junction': 'bookmark_tags',
        'dependents': [],  # No other tables reference bookmarks.id
    },
    'notes': {
        'junction': 'note_tags',
        'dependents': [('note_versions', 'note_id')],  # note_versions.note_id -> notes.id
    },
    'prompts': {
        'junction': 'prompt_tags',
        'dependents': [],
    },
}

# Tables without junction dependencies
SIMPLE_TABLES = ['tags', 'api_tokens']

# Tables that need ID mapping preserved for JSONB updates
# content_lists IDs are stored in user_settings.sidebar_order
TABLES_NEEDING_ID_MAPPING = ['content_lists']

# Global ID mappings: {table_name: {old_int_id: new_uuid}}
ID_MAPPINGS: dict[str, dict[int, str]] = {}


def get_pk_constraint_name(connection, table_name: str) -> str:
    """Query actual primary key constraint name from database."""
    result = connection.execute(sa.text("""
        SELECT constraint_name FROM information_schema.table_constraints
        WHERE table_name = :table AND constraint_type = 'PRIMARY KEY'
    """), {"table": table_name})
    return result.scalar()


def get_fk_constraint_name(connection, table_name: str, column_name: str) -> str:
    """Query actual foreign key constraint name from database."""
    result = connection.execute(sa.text("""
        SELECT tc.constraint_name
        FROM information_schema.table_constraints tc
        JOIN information_schema.key_column_usage kcu
            ON tc.constraint_name = kcu.constraint_name
        WHERE tc.table_name = :table
            AND tc.constraint_type = 'FOREIGN KEY'
            AND kcu.column_name = :column
    """), {"table": table_name, "column": column_name})
    return result.scalar()


def convert_table_with_junction(
    table_name: str,
    junction_table: str,
    dependent_tables: list[tuple[str, str]],
) -> None:
    """
    Convert a table, its junction table FK, and any dependent table FKs to UUID.

    Args:
        table_name: Main table to convert (e.g., 'notes')
        junction_table: Junction table for tags (e.g., 'note_tags')
        dependent_tables: List of (table, fk_column) that reference this table
                         (e.g., [('note_versions', 'note_id')])
    """
    connection = op.get_bind()
    fk_column = f"{table_name[:-1]}_id"  # bookmarks -> bookmark_id

    # Query actual constraint names (don't assume naming convention)
    pk_name = get_pk_constraint_name(connection, table_name)
    junction_fk_name = get_fk_constraint_name(connection, junction_table, fk_column)

    # 1. Add new UUID column to main table
    op.add_column(table_name, sa.Column('new_id', PG_UUID(as_uuid=True), nullable=True))

    # 2. Generate UUIDs for existing rows and build ID mapping
    id_mapping: dict[int, str] = {}
    rows = connection.execute(sa.text(f"SELECT id FROM {table_name}"))
    for row in rows:
        new_uuid = str(uuid7())
        id_mapping[row.id] = new_uuid
        connection.execute(
            sa.text(f"UPDATE {table_name} SET new_id = :new_id WHERE id = :old_id"),
            {"new_id": new_uuid, "old_id": row.id}
        )

    # 3. Make new_id non-nullable
    op.alter_column(table_name, 'new_id', nullable=False)

    # 4. Update junction table FK (while old id column still exists for joining)
    op.add_column(junction_table, sa.Column(f'new_{fk_column}', PG_UUID(as_uuid=True), nullable=True))
    connection.execute(sa.text(f"""
        UPDATE {junction_table} jt
        SET new_{fk_column} = t.new_id
        FROM {table_name} t
        WHERE jt.{fk_column} = t.id
    """))

    # 5. Update dependent tables FKs (while old id column still exists for joining)
    dependent_fk_names = {}
    for dep_table, dep_fk_col in dependent_tables:
        # Query actual FK constraint name
        dep_fk_name = get_fk_constraint_name(connection, dep_table, dep_fk_col)
        dependent_fk_names[(dep_table, dep_fk_col)] = dep_fk_name

        # Add new FK column
        op.add_column(dep_table, sa.Column(f'new_{dep_fk_col}', PG_UUID(as_uuid=True), nullable=True))

        # Copy FK references using join (old id still exists)
        connection.execute(sa.text(f"""
            UPDATE {dep_table} d
            SET new_{dep_fk_col} = t.new_id
            FROM {table_name} t
            WHERE d.{dep_fk_col} = t.id
        """))

    # 6. Drop old constraints
    op.drop_constraint(junction_fk_name, junction_table, type_='foreignkey')
    for dep_table, dep_fk_col in dependent_tables:
        op.drop_constraint(dependent_fk_names[(dep_table, dep_fk_col)], dep_table, type_='foreignkey')
    op.drop_constraint(pk_name, table_name, type_='primary')

    # 7. Drop old columns
    op.drop_column(junction_table, fk_column)
    for dep_table, dep_fk_col in dependent_tables:
        op.drop_column(dep_table, dep_fk_col)
    op.drop_column(table_name, 'id')

    # 8. Rename new columns
    op.alter_column(table_name, 'new_id', new_column_name='id')
    op.alter_column(junction_table, f'new_{fk_column}', new_column_name=fk_column)
    for dep_table, dep_fk_col in dependent_tables:
        op.alter_column(dep_table, f'new_{dep_fk_col}', new_column_name=dep_fk_col)

    # 9. Recreate constraints
    op.create_primary_key(f'{table_name}_pkey', table_name, ['id'])
    op.create_foreign_key(
        f'{junction_table}_{fk_column}_fkey', junction_table,
        table_name, [fk_column], ['id'], ondelete='CASCADE'
    )
    for dep_table, dep_fk_col in dependent_tables:
        op.create_foreign_key(
            f'{dep_table}_{dep_fk_col}_fkey', dep_table,
            table_name, [dep_fk_col], ['id'], ondelete='CASCADE'
        )


def convert_simple_table(table_name: str, store_mapping: bool = False) -> None:
    """Convert a table without junction dependencies to UUID."""
    connection = op.get_bind()

    # Query actual constraint name (don't assume naming convention)
    pk_name = get_pk_constraint_name(connection, table_name)

    # 1. Add new UUID column
    op.add_column(table_name, sa.Column('new_id', PG_UUID(as_uuid=True), nullable=True))

    # 2. Generate UUIDs for existing rows (and optionally store mapping)
    id_mapping: dict[int, str] = {}
    rows = connection.execute(sa.text(f"SELECT id FROM {table_name}"))
    for row in rows:
        new_uuid = str(uuid7())
        if store_mapping:
            id_mapping[row.id] = new_uuid
        connection.execute(
            sa.text(f"UPDATE {table_name} SET new_id = :new_id WHERE id = :old_id"),
            {"new_id": new_uuid, "old_id": row.id}
        )

    # Store mapping globally if needed for JSONB updates
    if store_mapping:
        ID_MAPPINGS[table_name] = id_mapping

    # 3. Make non-nullable
    op.alter_column(table_name, 'new_id', nullable=False)

    # 4. Drop old PK (using actual name from database)
    op.drop_constraint(pk_name, table_name, type_='primary')

    # 5. Drop old column
    op.drop_column(table_name, 'id')

    # 6. Rename new column
    op.alter_column(table_name, 'new_id', new_column_name='id')

    # 7. Recreate PK
    op.create_primary_key(f'{table_name}_pkey', table_name, ['id'])


def migrate_sidebar_order_jsonb() -> None:
    """
    Update user_settings.sidebar_order JSONB to use new content_list UUIDs.

    The sidebar_order field stores list IDs as integers:
    {"items": [{"type": "list", "id": 3}, ...]}

    This function updates those integer IDs to the new UUIDs using the
    ID_MAPPINGS['content_lists'] mapping built during content_lists migration.
    """
    connection = op.get_bind()
    list_id_mapping = ID_MAPPINGS.get('content_lists', {})

    if not list_id_mapping:
        return  # No content_lists to migrate

    # Fetch all user_settings with sidebar_order
    rows = connection.execute(sa.text(
        "SELECT user_id, sidebar_order FROM user_settings WHERE sidebar_order IS NOT NULL"
    ))

    for row in rows:
        sidebar_order = row.sidebar_order
        if not sidebar_order or 'items' not in sidebar_order:
            continue

        updated = False
        items = sidebar_order.get('items', [])

        def update_items(items_list: list) -> bool:
            """Recursively update list IDs in items (handles groups with nested items)."""
            changed = False
            for item in items_list:
                if item.get('type') == 'list' and isinstance(item.get('id'), int):
                    old_id = item['id']
                    if old_id in list_id_mapping:
                        item['id'] = list_id_mapping[old_id]
                        changed = True
                elif item.get('type') == 'group' and 'items' in item:
                    if update_items(item['items']):
                        changed = True
            return changed

        if update_items(items):
            # Update the row with new JSONB
            import json
            connection.execute(
                sa.text("UPDATE user_settings SET sidebar_order = :new_order WHERE user_id = :user_id"),
                {"new_order": json.dumps(sidebar_order), "user_id": row.user_id}
            )


def upgrade() -> None:
    # 1. Convert tables with junction dependencies (and their dependent tables)
    for table_name, config in TABLES_WITH_JUNCTIONS.items():
        convert_table_with_junction(
            table_name,
            config['junction'],
            config['dependents'],
        )

    # 2. Convert content_lists with ID mapping (needed for JSONB update)
    convert_simple_table('content_lists', store_mapping=True)

    # 3. Convert remaining simple tables
    for table_name in SIMPLE_TABLES:
        convert_simple_table(table_name)

    # 4. Convert note_versions PK (FK already updated in step 1 with notes)
    convert_simple_table('note_versions')

    # 5. Update JSONB fields that store entity IDs
    migrate_sidebar_order_jsonb()

    # 6. Drop deprecated tab_order column (replaced by sidebar_order)
    op.drop_column('user_settings', 'tab_order')

    # 7. Recreate any dropped indexes
    # bookmarks: partial unique index on user_id + url
    op.create_index(
        'uq_bookmark_user_url_active', 'bookmarks',
        ['user_id', 'url'], unique=True,
        postgresql_where=sa.text("deleted_at IS NULL")
    )

    # prompts: partial unique index on user_id + name
    op.create_index(
        'uq_prompt_user_name_active', 'prompts',
        ['user_id', 'name'], unique=True,
        postgresql_where=sa.text("deleted_at IS NULL")
    )

    # Junction table indexes (tag_id lookups)
    op.create_index('ix_bookmark_tags_tag_id', 'bookmark_tags', ['tag_id'])
    op.create_index('ix_note_tags_tag_id', 'note_tags', ['tag_id'])
    op.create_index('ix_prompt_tags_tag_id', 'prompt_tags', ['tag_id'])

    # note_versions composite index
    op.create_index('ix_note_versions_note_id_version', 'note_versions', ['note_id', 'version'])


def downgrade() -> None:
    # Downgrade not supported - restore from backup instead
    raise NotImplementedError(
        "Downgrade not supported for PK type change. Restore from backup."
    )
```

**Important notes for the agent:**
- The migration queries actual constraint names from the database (no hardcoded assumptions)
- The `tags` table has a unique constraint on `(user_id, name)` - preserved since only `id` column changes
- The `prompts` table has a partial unique index `uq_prompt_user_name_active` - recreated in step 6
- The `sidebar_order` JSONB field stores list IDs - migrated to UUIDs in step 5
- Test thoroughly against backup before production

### Testing Strategy

**Local testing after migration:**

After running the migration locally (`make migrate`), verify the migration worked correctly:

```bash
# 1. Verify all tables have UUID PKs
psql "$DATABASE_URL" -c "
  SELECT table_name, column_name, data_type
  FROM information_schema.columns
  WHERE table_name IN ('bookmarks', 'notes', 'prompts', 'content_lists', 'tags', 'api_tokens', 'note_versions')
    AND column_name = 'id'
  ORDER BY table_name;"
# All should show data_type = 'uuid'

# 2. Verify no NULL IDs
psql "$DATABASE_URL" -c "
  SELECT 'bookmarks' as tbl, COUNT(*) as null_ids FROM bookmarks WHERE id IS NULL
  UNION ALL SELECT 'notes', COUNT(*) FROM notes WHERE id IS NULL
  UNION ALL SELECT 'prompts', COUNT(*) FROM prompts WHERE id IS NULL
  UNION ALL SELECT 'content_lists', COUNT(*) FROM content_lists WHERE id IS NULL
  UNION ALL SELECT 'tags', COUNT(*) FROM tags WHERE id IS NULL
  UNION ALL SELECT 'api_tokens', COUNT(*) FROM api_tokens WHERE id IS NULL
  UNION ALL SELECT 'note_versions', COUNT(*) FROM note_versions WHERE id IS NULL;"
# All should return 0

# 3. Verify junction table FK integrity
psql "$DATABASE_URL" -c "
  SELECT 'bookmark_tags' as tbl, COUNT(*) as orphaned
  FROM bookmark_tags bt LEFT JOIN bookmarks b ON bt.bookmark_id = b.id WHERE b.id IS NULL
  UNION ALL
  SELECT 'note_tags', COUNT(*)
  FROM note_tags nt LEFT JOIN notes n ON nt.note_id = n.id WHERE n.id IS NULL
  UNION ALL
  SELECT 'prompt_tags', COUNT(*)
  FROM prompt_tags pt LEFT JOIN prompts p ON pt.prompt_id = p.id WHERE p.id IS NULL
  UNION ALL
  SELECT 'note_versions', COUNT(*)
  FROM note_versions nv LEFT JOIN notes n ON nv.note_id = n.id WHERE n.id IS NULL;"
# All should return 0

# 4. Verify indexes exist
psql "$DATABASE_URL" -c "
  SELECT indexname FROM pg_indexes
  WHERE tablename IN ('bookmarks', 'notes', 'prompts', 'bookmark_tags', 'note_tags', 'prompt_tags', 'note_versions')
  ORDER BY tablename, indexname;"
# Should include: uq_bookmark_user_url_active, uq_prompt_user_name_active,
#                 ix_bookmark_tags_tag_id, ix_note_tags_tag_id,
#                 ix_prompt_tags_tag_id, ix_note_versions_note_id_version

# 5. Verify sidebar_order JSONB has UUID strings (not integers) for list IDs
psql "$DATABASE_URL" -c "
  SELECT user_id, sidebar_order
  FROM user_settings
  WHERE sidebar_order IS NOT NULL
  LIMIT 5;"
# List IDs should be UUID strings like '01938a12-3b45-7c67-8d90-ef1234567890', not integers
```

**Note:** Full test suite (`make tests`) runs in Milestone 3 after code is updated. Production backup testing is in Milestone 6.

### Dependencies
Milestone 1 (UUIDv7Mixin must exist)

### Risk Factors
- **Backup required**: Always backup before running on production
- **No downgrade**: Rollback means restoring from backup

---

## Milestone 3: Update Backend Code

### Goal
Update all models, schemas, routers, and services to use UUID strings instead of integers.

### Success Criteria
- All 6 models use `UUIDv7Mixin`
- All schemas have `id: str`
- All routers have `str` path parameters
- All services have updated type hints
- All existing tests pass (with updates for string IDs)

### Key Changes

**1. Update models to use UUIDv7Mixin:**

Remove `id: Mapped[int] = mapped_column(primary_key=True)` from each model and add `UUIDv7Mixin`:

```python
# backend/src/models/bookmark.py
from models.base import ArchivableMixin, Base, TimestampMixin, UUIDv7Mixin

class Bookmark(Base, TimestampMixin, ArchivableMixin, UUIDv7Mixin):
    __tablename__ = "bookmarks"
    # Remove: id: Mapped[int] = mapped_column(primary_key=True)
    # ... rest unchanged
```

Apply same pattern to:
- `backend/src/models/note.py`
- `backend/src/models/prompt.py`
- `backend/src/models/content_list.py`
- `backend/src/models/tag.py`
- `backend/src/models/api_token.py`
- `backend/src/models/note_version.py` (also update `note_id` type to `UUID`)

**Cleanup deprecated column:**

Remove `tab_order` from `backend/src/models/user_settings.py` (column dropped in migration):

```python
# Remove this entire field definition:
# tab_order: Mapped[dict | None] = mapped_column(
#     JSONB,
#     nullable=True,
#     comment="DEPRECATED - Use sidebar_order instead",
# )
```

**2. Update junction table definitions in `backend/src/models/tag.py`:**

```python
from sqlalchemy.dialects.postgresql import UUID as PG_UUID

bookmark_tags = Table(
    "bookmark_tags",
    Base.metadata,
    Column(
        "bookmark_id",
        PG_UUID(as_uuid=True),  # Changed from Integer
        ForeignKey("bookmarks.id", ondelete="CASCADE"),
        primary_key=True,
    ),
    Column(
        "tag_id",
        PG_UUID(as_uuid=True),  # Changed from Integer
        ForeignKey("tags.id", ondelete="CASCADE"),
        primary_key=True,
    ),
    Index("ix_bookmark_tags_tag_id", "tag_id"),
)
# Same for note_tags and prompt_tags
```

**3. Update schemas - change `id: int` to `id: UUID`:**

Using `UUID` type in schemas provides:
- Type safety (consistent with routers/services)
- Automatic serialization to string in JSON responses
- Validation on input (malformed UUIDs rejected)

Files to update:
- `backend/src/schemas/bookmark.py`: `BookmarkListItem`, `BookmarkResponse`
- `backend/src/schemas/note.py`: `NoteListItem`, `NoteResponse`
- `backend/src/schemas/prompt.py`: `PromptListItem`, `PromptResponse`
- `backend/src/schemas/content_list.py`: `ContentListResponse`
- `backend/src/schemas/tag.py`: `TagResponse`
- `backend/src/schemas/token.py`: `TokenResponse`
- `backend/src/schemas/sidebar.py`: `SidebarListItem.id: int` → `UUID` (also affects `SidebarListItemComputed`)

Example:
```python
from uuid import UUID

class BookmarkListItem(BaseModel):
    model_config = ConfigDict(from_attributes=True)

    id: UUID  # Changed from int - Pydantic auto-serializes to string in JSON
    # ... rest unchanged
```

**4. Update routers - use UUID type for path parameters:**

Using `UUID` type directly provides automatic 422 validation on malformed UUIDs (instead of letting invalid strings propagate to the database):

```python
from uuid import UUID

@router.get("/{bookmark_id}")
async def get_bookmark(bookmark_id: UUID) -> BookmarkResponse:
    # FastAPI automatically returns 422 if bookmark_id is not a valid UUID
    ...
```

Files to update:
- `backend/src/api/routers/bookmarks.py`: `bookmark_id: int` → `bookmark_id: UUID`
- `backend/src/api/routers/notes.py`: `note_id: int` → `note_id: UUID`
- `backend/src/api/routers/prompts.py`: `prompt_id: int` → `prompt_id: UUID`
- `backend/src/api/routers/lists.py`: `list_id: int` → `list_id: UUID`
- `backend/src/api/routers/tags.py`: `tag_id: int` → `tag_id: UUID`
- `backend/src/api/routers/tokens.py`: `token_id: int` → `token_id: UUID`

Also update `list_id` query parameters in bookmarks, notes, prompts routers to `UUID | None`.

**5. Update services - change type hints:**

Files to update:
- `backend/src/services/bookmark_service.py`
- `backend/src/services/note_service.py`
- `backend/src/services/prompt_service.py`
- `backend/src/services/list_service.py`
- `backend/src/services/tag_service.py`
- `backend/src/services/token_service.py`

Example:
```python
from uuid import UUID

async def get_bookmark(
    session: AsyncSession,
    user_id: int,  # user_id stays int (users table not migrated)
    bookmark_id: UUID,  # Changed from int
    ...
) -> Bookmark | None:
```

**6. Update TaggableEntity Protocol in `backend/src/services/base_entity_service.py`:**

```python
from uuid import UUID

class TaggableEntity(Protocol):
    """Protocol defining the interface for entities that support tagging and soft-delete."""

    id: UUID  # Changed from int
    user_id: int  # Stays int (users table not migrated)
    # ... rest unchanged
```

Also update `entity_id: int` parameters in `BaseEntityService` methods to `entity_id: UUID`.

### Testing Strategy

**Update existing tests:**

```bash
# Find all tests that need updates
grep -rn "\.id == [0-9]" backend/tests/
grep -rn '"id": [0-9]' backend/tests/
grep -rn "_id: int" backend/tests/
grep -rn "bookmark_id=" backend/tests/
grep -rn "note_id=" backend/tests/
grep -rn "prompt_id=" backend/tests/
```

Common patterns to update:
- `assert response.json()["id"] == 1` → `assert isinstance(response.json()["id"], str)` and validate UUID format
- `bookmark_id: int` → `bookmark_id: UUID`
- Integer comparisons → UUID comparisons

**New tests to add:**
- Test 422 returned for malformed UUID path parameters (e.g., `/bookmarks/not-a-uuid`)
- Test 404 returned for valid UUID that doesn't exist

**Run full test suite:**
```bash
make tests  # Runs linting + all backend + frontend tests
```

### Dependencies
Milestone 2 (database must be migrated)

### Risk Factors
- Many files to update - use grep to find all occurrences
- Test fixtures may create entities with hardcoded integer IDs

---

## Milestone 4: Update MCP Servers

### Goal
Update MCP servers to use UUID strings for ID parameters.

### Success Criteria
- MCP tools accept `id` parameters as `str`
- MCP server tests pass

### Key Changes

**1. Update `backend/src/mcp_server/server.py`:**

```python
# get_bookmark
bookmark_id: Annotated[str, Field(description="The UUID of the bookmark to retrieve")]

# get_note
note_id: Annotated[str, Field(description="The UUID of the note to retrieve")]
```

**2. Update `backend/src/prompt_mcp_server/server.py`:**

Similar changes for any prompt ID parameters.

### Testing Strategy
- Test `get_bookmark` and `get_note` work with UUID string IDs
- Test 404 returned for invalid/nonexistent UUID
- Run MCP server tests

### Dependencies
Milestone 3 (backend code must be updated)

### Risk Factors
Very low risk - minimal code changes

---

## Milestone 5: Update Frontend

### Goal
Update frontend TypeScript types to expect `id` as string.

### Success Criteria
- TypeScript types use `id: string` instead of `id: number`
- All existing code using `bookmark.id`, `note.id`, etc. continues working
- URL routes continue working
- All frontend tests pass
- TypeScript compiles without errors

### Key Changes

**1. Update `frontend/src/types.ts`:**

```typescript
export interface BookmarkListItem {
  id: string  // Changed from number
  // ... rest unchanged
}

export interface NoteListItem {
  id: string  // Changed from number
  // ...
}

export interface PromptListItem {
  id: string  // Changed from number
  // ...
}

export interface ContentList {
  id: string  // Changed from number
  // ...
}

export interface Tag {
  id: string  // Changed from number
  // ...
}

export interface Token {
  id: string  // Changed from number
  // ...
}

export interface ContentListItem {
  id: string  // Changed from number
  // ...
}

export interface SidebarListItem {
  type: 'list'
  id: string  // Changed from number
}

// Update query param types
export interface NoteSearchParams {
  // ...
  list_id?: string  // Changed from number
}

export interface BookmarkSearchParams {
  // ...
  list_id?: string  // Changed from number
}

export interface PromptSearchParams {
  // ...
  list_id?: string  // Changed from number
}

export interface ContentSearchParams {
  // ...
  list_id?: string  // Changed from number
}
```

**2. Search for any numeric operations on IDs:**

```bash
cd frontend
grep -rn "\.id ===" src/
grep -rn "\.id ==" src/
grep -rn "\.id <" src/
grep -rn "\.id >" src/
grep -rn "parseInt.*id" src/
grep -rn "Number.*id" src/
```

Fix any code that treats IDs as numbers.

### Testing Strategy

```bash
cd frontend

# TypeScript compilation
npm run build

# Run tests
npm run test:run

# Linting
npm run lint
```

Manual testing:
- Create, read, update, delete bookmarks/notes/prompts
- Navigate via URLs
- Test list filtering with `list_id`

### Dependencies
All backend milestones complete

### Risk Factors
- Any code comparing IDs as numbers would break
- Any code parsing IDs as integers would break

---

## Milestone 6: Pre-Production Verification

### Goal
Test the migration against a copy of production data before deploying. This is the final gate before production.

### Success Criteria
- Migration runs successfully on production backup
- All verification checks pass
- No data loss or corruption

### Prerequisites
- All previous milestones complete
- All tests passing (`make tests`)
- Frontend builds and tests pass

### Steps

**1. Clone production database to backup:**

```bash
# Set environment variables
export PROD_DATABASE_URL="postgresql://postgres:..."
export BACKUP_DATABASE_URL="postgresql://postgres:..."

# Copy production database
PGSSLMODE=require pg_dump --dbname="$PROD_DATABASE_URL" -F c \
  | PGSSLMODE=require pg_restore --clean --if-exists --no-owner --no-privileges \
    --dbname="$BACKUP_DATABASE_URL"
```

**2. Record pre-migration row counts:**

```bash
PGSSLMODE=require psql "$BACKUP_DATABASE_URL" -c "
  SELECT 'bookmarks' as tbl, COUNT(*) FROM bookmarks
  UNION ALL SELECT 'notes', COUNT(*) FROM notes
  UNION ALL SELECT 'prompts', COUNT(*) FROM prompts
  UNION ALL SELECT 'content_lists', COUNT(*) FROM content_lists
  UNION ALL SELECT 'tags', COUNT(*) FROM tags
  UNION ALL SELECT 'api_tokens', COUNT(*) FROM api_tokens
  UNION ALL SELECT 'note_versions', COUNT(*) FROM note_versions
  UNION ALL SELECT 'bookmark_tags', COUNT(*) FROM bookmark_tags
  UNION ALL SELECT 'note_tags', COUNT(*) FROM note_tags
  UNION ALL SELECT 'prompt_tags', COUNT(*) FROM prompt_tags;"
```

**3. Run migration:**

```bash
DATABASE_URL="$BACKUP_DATABASE_URL" uv run alembic upgrade head
```

**4. Run verification checks:**

```bash
# Verify all tables have UUID PKs
PGSSLMODE=require psql "$BACKUP_DATABASE_URL" -c "
  SELECT table_name, column_name, data_type
  FROM information_schema.columns
  WHERE table_name IN ('bookmarks', 'notes', 'prompts', 'content_lists', 'tags', 'api_tokens', 'note_versions')
    AND column_name = 'id'
  ORDER BY table_name;"
# All should show data_type = 'uuid'

# Verify no NULL IDs
PGSSLMODE=require psql "$BACKUP_DATABASE_URL" -c "
  SELECT 'bookmarks' as tbl, COUNT(*) as null_ids FROM bookmarks WHERE id IS NULL
  UNION ALL SELECT 'notes', COUNT(*) FROM notes WHERE id IS NULL
  UNION ALL SELECT 'prompts', COUNT(*) FROM prompts WHERE id IS NULL
  UNION ALL SELECT 'content_lists', COUNT(*) FROM content_lists WHERE id IS NULL
  UNION ALL SELECT 'tags', COUNT(*) FROM tags WHERE id IS NULL
  UNION ALL SELECT 'api_tokens', COUNT(*) FROM api_tokens WHERE id IS NULL
  UNION ALL SELECT 'note_versions', COUNT(*) FROM note_versions WHERE id IS NULL;"
# All should return 0

# Verify FK integrity
PGSSLMODE=require psql "$BACKUP_DATABASE_URL" -c "
  SELECT 'bookmark_tags' as tbl, COUNT(*) as orphaned
  FROM bookmark_tags bt LEFT JOIN bookmarks b ON bt.bookmark_id = b.id WHERE b.id IS NULL
  UNION ALL
  SELECT 'note_tags', COUNT(*)
  FROM note_tags nt LEFT JOIN notes n ON nt.note_id = n.id WHERE n.id IS NULL
  UNION ALL
  SELECT 'prompt_tags', COUNT(*)
  FROM prompt_tags pt LEFT JOIN prompts p ON pt.prompt_id = p.id WHERE p.id IS NULL
  UNION ALL
  SELECT 'note_versions', COUNT(*)
  FROM note_versions nv LEFT JOIN notes n ON nv.note_id = n.id WHERE n.id IS NULL;"
# All should return 0

# Verify sidebar_order JSONB has UUID strings for list IDs
PGSSLMODE=require psql "$BACKUP_DATABASE_URL" -c "
  SELECT user_id, sidebar_order
  FROM user_settings
  WHERE sidebar_order IS NOT NULL
  LIMIT 5;"
# List IDs should be UUID strings, not integers

# Verify row counts match pre-migration (re-run step 2 query and compare)
```

**5. Test API against backup (optional but recommended):**

```bash
# Point local API at backup database
DATABASE_URL="$BACKUP_DATABASE_URL" make run

# In another terminal, run a quick smoke test
curl http://localhost:8000/health
# Test a few endpoints manually to verify data looks correct
```

### Dependencies
Milestones 1-5 complete

### Risk Factors
- Ensure backup database is isolated (not accidentally pointing at production)
- Time the migration to estimate production downtime

---

## Summary

| Milestone | Component | Scope |
|-----------|-----------|-------|
| 1 | Foundation | `uuid7` library + `UUIDv7Mixin` |
| 2 | Database Migration | Single migration for all 7 tables + 3 junction tables |
| 3 | Backend Code | Models, schemas, routers, services |
| 4 | MCP Servers | ID parameter types |
| 5 | Frontend | TypeScript types |
| 6 | Pre-Production Verification | Test migration on production backup |

Total: 6 milestones. Each milestone should be reviewed before proceeding to the next.

### Migration Workflow

1. **Milestone 1**: Add library and mixin (no DB changes)
2. **Milestone 2**: Write migration, run locally, verify with SQL checks
3. **Milestone 3**: Update backend code, run `make tests`
4. **Milestone 4**: Update MCP servers
5. **Milestone 5**: Update frontend, run `npm run build && npm run test:run`
6. **Milestone 6**: Clone production DB → run migration → verify → smoke test API
7. **Deploy**: Run migration on production, deploy new code

### Important Notes

- **No backwards compatibility**: Old integer IDs will not work after migration
- **API breaking change**: Clients must update to expect string IDs (UUID format)
- **Database backup required**: Test migration on backup before production
- **Downgrade not supported**: Rollback means restoring from pre-migration backup
- **Atomic migration**: All tables converted in single transaction
- **JSONB migration**: `sidebar_order` field updated to use UUID strings for list IDs
- **Cleanup**: Deprecated `tab_order` column dropped from `user_settings` (replaced by `sidebar_order`)
