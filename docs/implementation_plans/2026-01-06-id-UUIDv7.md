# UUIDv7 Primary Key Implementation Plan

## Overview

Replace sequential integer primary keys with UUIDv7 for all content tables. This eliminates information leakage from auto-increment IDs while providing time-sortable, globally unique identifiers.

### Problem

Current auto-increment integer IDs leak information:
- Competitors can infer total record counts ("my bookmark is ID 50,000")
- Growth rates can be estimated by creating accounts over time
- Sequential IDs provide enumeration surface for attackers

### Solution: UUIDv7 as Sole Primary Key

Replace integer PKs with UUIDv7:
- **Single ID**: One `id` column that is both the PK and API identifier
- **Time-sortable**: UUIDv7 embeds a Unix timestamp (millisecond precision)
- **Native PostgreSQL**: Uses `uuid` type (16 bytes binary, efficient indexing)
- **No information leakage**: Random component prevents inference
- **Distributed-ready**: IDs can be generated on any node without coordination (enables future sharding, multi-region, microservices)

### Why UUIDv7 Over Other Options

| Option | Storage | Time-sortable | Native PG type | Standard |
|--------|---------|---------------|----------------|----------|
| UUIDv4 | 16 bytes | No | Yes | RFC 4122 |
| UUIDv7 | 16 bytes | Yes | Yes | RFC 9562 |
| ULID | 26 bytes (string) | Yes | No | Informal |
| Dual ID | 4+26 bytes | N/A | N/A | N/A |

**UUIDv7 wins because:**
1. Native PostgreSQL `uuid` type = smaller storage, better indexing than string-based ULID
2. Time-sortable = sequential insert performance (unlike UUIDv4)
3. RFC 9562 standard = better interoperability
4. Simpler than dual-ID approach = less code, fewer migrations, no mapping logic

### Tables to Migrate

Based on API analysis, these tables expose integer IDs:

| Table | API Exposure | Foreign Keys |
|-------|--------------|--------------|
| `bookmarks` | `/bookmarks/{id}`, responses | `user_id`, junction tables |
| `notes` | `/notes/{id}`, responses | `user_id`, junction tables |
| `prompts` | `/prompts/{id}`, responses | `user_id`, junction tables |
| `content_lists` | `/lists/{id}`, responses, `list_id` query params | `user_id` |
| `tags` | Tag responses, rename endpoint | `user_id`, junction tables |
| `api_tokens` | `/tokens/{id}`, responses | `user_id` |
| `note_versions` | Internal (future use) | `note_id` FK to notes |

**Tables NOT migrated:**
- `users`: Internal-only ID (Auth0 users never see it; exposed ID is `auth0_id`)
- `user_consent`: Internal-only (consent endpoint returns consent data, not ID)
- `user_settings`: Internal-only (settings endpoint returns settings, not ID)
- Junction tables (`bookmark_tags`, etc.): Will be updated to reference new UUIDs

### Key Design Decisions

1. **UUID generation**: Generate in Python at model instantiation using `uuid7` library
2. **Field name unchanged**: API continues using `id` - only the type changes (`int` → `str`)
3. **URL format**: UUIDs with hyphens are URL-safe (e.g., `/bookmarks/01938a12-3b45-7c67-8d90-ef1234567890`)
4. **Minimal breaking change**: Only the type changes; clients using `/bookmarks/${bookmark.id}` continue working
5. **Single migration**: All tables migrated in one atomic transaction

### Documentation

Read before implementing:
- UUIDv7 RFC 9562: https://www.rfc-editor.org/rfc/rfc9562#name-uuid-version-7
- uuid7 Python library: https://pypi.org/project/uuid7/

---

## Milestone 1: Add UUID7 Library and Create ID Mixin

### Goal
Add UUIDv7 generation capability and create a mixin that replaces integer PKs.

### Success Criteria
- `uuid7` library installed
- `UUIDv7Mixin` created that defines `id` as UUID primary key
- UUID auto-generates on model instantiation
- Unit tests verify UUID generation and format

### Key Changes

**1. Add dependency:**

```bash
uv add uuid7
```

**2. Create `UUIDv7Mixin` in `backend/src/models/base.py`:**

```python
from uuid import UUID
from uuid7 import uuid7
from sqlalchemy.dialects.postgresql import UUID as PG_UUID

class UUIDv7Mixin:
    """Mixin that provides a UUIDv7 primary key."""

    id: Mapped[UUID] = mapped_column(
        PG_UUID(as_uuid=True),
        primary_key=True,
        default=uuid7,
    )
```

Notes:
- `uuid7()` returns a standard `uuid.UUID` object
- `as_uuid=True` means SQLAlchemy handles UUID↔string conversion automatically
- No need for `str(uuid7())` - the type is native

### Testing Strategy
- Test UUID is auto-generated when not provided
- Test UUID format is valid (version 7, correct structure)
- Test UUIDs are time-ordered (create two in sequence, confirm ordering)
- Test custom UUID can be provided (for testing/seeding)

### Dependencies
None - this is the foundation.

### Risk Factors
- Verify `default=uuid7` works correctly with SQLAlchemy (function reference, not call)

---

## Milestone 2: Database Migration (All Tables)

### Goal
Single Alembic migration that converts all 7 tables from integer PKs to UUIDv7 PKs, including updating all 3 junction table foreign keys.

### Success Criteria
- All tables converted: `bookmarks`, `notes`, `prompts`, `content_lists`, `tags`, `api_tokens`, `note_versions`
- All junction tables updated: `bookmark_tags`, `note_tags`, `prompt_tags`
- `note_versions.note_id` FK updated to reference new `notes.id` UUID
- Migration is atomic (all or nothing)
- Migration tested against production backup
- Rollback documented (restore from backup)

### Key Changes

**Create single Alembic migration:**

```bash
make migration message="convert all content tables to uuid7 primary keys"
```

**Migration script structure:**

The migration uses a **phased approach** to avoid FK constraint conflicts and ensure all composite PKs are properly recreated:

| Phase | Description |
|-------|-------------|
| 1 | Add `new_id` columns to ALL tables (entity PKs + junction FKs) |
| 2 | Populate all new columns with UUIDs and FK references |
| 3 | Drop ALL old constraints (PKs, FKs, unique indexes) |
| 4 | Drop old columns, rename new columns |
| 5 | Recreate ALL constraints |

This approach ensures:
- No FK conflicts during type changes (all constraints dropped before column changes)
- All composite PKs on junction tables are properly recreated
- Atomic transaction covers everything

```python
"""Convert all content tables to UUID7 primary keys.

Tables converted:
- bookmarks, notes, prompts, content_lists, tags, api_tokens, note_versions

Junction tables updated:
- bookmark_tags (bookmark_id, tag_id)
- note_tags (note_id, tag_id)
- prompt_tags (prompt_id, tag_id)

Other updates:
- note_versions.note_id FK
- user_settings.sidebar_order JSONB (list IDs)

Cleanup:
- Drop deprecated tab_order column from user_settings

Revision ID: xxxx
"""
import json
from uuid7 import uuid7
from sqlalchemy.dialects.postgresql import UUID as PG_UUID
import sqlalchemy as sa
from alembic import op

# =============================================================================
# Configuration
# =============================================================================

# Entity tables with their PKs to convert
ENTITY_TABLES = ['bookmarks', 'notes', 'prompts', 'content_lists', 'tags', 'api_tokens', 'note_versions']

# Junction tables: {table: [(fk_column, referenced_table), ...]}
JUNCTION_TABLES = {
    'bookmark_tags': [('bookmark_id', 'bookmarks'), ('tag_id', 'tags')],
    'note_tags': [('note_id', 'notes'), ('tag_id', 'tags')],
    'prompt_tags': [('prompt_id', 'prompts'), ('tag_id', 'tags')],
}

# Other FK relationships: [(table, fk_column, referenced_table), ...]
OTHER_FKS = [
    ('note_versions', 'note_id', 'notes'),
]

# Global ID mappings for JSONB updates: {table_name: {old_int_id: new_uuid_str}}
ID_MAPPINGS: dict[str, dict[int, str]] = {}

# =============================================================================
# Helper Functions
# =============================================================================

def get_pk_constraint_name(connection, table_name: str) -> str | None:
    """Query actual primary key constraint name from database."""
    result = connection.execute(sa.text("""
        SELECT constraint_name FROM information_schema.table_constraints
        WHERE table_name = :table AND constraint_type = 'PRIMARY KEY'
    """), {"table": table_name})
    return result.scalar()


def get_fk_constraint_name(connection, table_name: str, column_name: str) -> str | None:
    """Query actual foreign key constraint name from database."""
    result = connection.execute(sa.text("""
        SELECT tc.constraint_name
        FROM information_schema.table_constraints tc
        JOIN information_schema.key_column_usage kcu
            ON tc.constraint_name = kcu.constraint_name
            AND tc.table_schema = kcu.table_schema
        WHERE tc.table_name = :table
            AND tc.constraint_type = 'FOREIGN KEY'
            AND kcu.column_name = :column
    """), {"table": table_name, "column": column_name})
    return result.scalar()


def get_unique_constraint_names(connection, table_name: str) -> list[str]:
    """Query all unique constraint names for a table."""
    result = connection.execute(sa.text("""
        SELECT constraint_name FROM information_schema.table_constraints
        WHERE table_name = :table AND constraint_type = 'UNIQUE'
    """), {"table": table_name})
    return [row[0] for row in result]


def get_index_names(connection, table_name: str) -> list[str]:
    """Query all index names for a table (excluding PK/unique constraint indexes)."""
    result = connection.execute(sa.text("""
        SELECT indexname FROM pg_indexes
        WHERE tablename = :table
          AND indexname NOT IN (
              SELECT constraint_name FROM information_schema.table_constraints
              WHERE table_name = :table
          )
    """), {"table": table_name})
    return [row[0] for row in result]


# =============================================================================
# Phase Functions
# =============================================================================

def phase1_add_new_columns(connection) -> None:
    """Phase 1: Add new_id columns to all entity tables and new FK columns to junction/dependent tables."""

    # Add new_id to all entity tables
    for table in ENTITY_TABLES:
        op.add_column(table, sa.Column('new_id', PG_UUID(as_uuid=True), nullable=True))

    # Add new FK columns to junction tables
    for junction_table, fk_columns in JUNCTION_TABLES.items():
        for fk_col, _ in fk_columns:
            op.add_column(junction_table, sa.Column(f'new_{fk_col}', PG_UUID(as_uuid=True), nullable=True))

    # Add new FK columns to other dependent tables
    for dep_table, fk_col, _ in OTHER_FKS:
        op.add_column(dep_table, sa.Column(f'new_{fk_col}', PG_UUID(as_uuid=True), nullable=True))


def phase2_populate_new_columns(connection) -> None:
    """Phase 2: Generate UUIDs for entity tables and populate FK references."""

    # Generate UUIDs for all entity tables
    for table in ENTITY_TABLES:
        # Store mapping for content_lists (needed for JSONB update)
        store_mapping = (table == 'content_lists')
        if store_mapping:
            ID_MAPPINGS[table] = {}

        rows = connection.execute(sa.text(f"SELECT id FROM {table}"))
        for row in rows:
            new_uuid = str(uuid7())
            if store_mapping:
                ID_MAPPINGS[table][row.id] = new_uuid
            connection.execute(
                sa.text(f"UPDATE {table} SET new_id = :new_id WHERE id = :old_id"),
                {"new_id": new_uuid, "old_id": row.id}
            )

        # Make new_id non-nullable
        op.alter_column(table, 'new_id', nullable=False)

    # Populate junction table FK columns using JOINs
    for junction_table, fk_columns in JUNCTION_TABLES.items():
        for fk_col, ref_table in fk_columns:
            connection.execute(sa.text(f"""
                UPDATE {junction_table} jt
                SET new_{fk_col} = t.new_id
                FROM {ref_table} t
                WHERE jt.{fk_col} = t.id
            """))

    # Populate other dependent table FK columns
    for dep_table, fk_col, ref_table in OTHER_FKS:
        connection.execute(sa.text(f"""
            UPDATE {dep_table} d
            SET new_{fk_col} = t.new_id
            FROM {ref_table} t
            WHERE d.{fk_col} = t.id
        """))


def phase3_drop_old_constraints(connection) -> None:
    """Phase 3: Drop all old constraints (FKs, PKs, indexes)."""

    # Drop FK constraints from junction tables
    for junction_table, fk_columns in JUNCTION_TABLES.items():
        for fk_col, _ in fk_columns:
            fk_name = get_fk_constraint_name(connection, junction_table, fk_col)
            if fk_name:
                op.drop_constraint(fk_name, junction_table, type_='foreignkey')

    # Drop FK constraints from other dependent tables
    for dep_table, fk_col, _ in OTHER_FKS:
        fk_name = get_fk_constraint_name(connection, dep_table, fk_col)
        if fk_name:
            op.drop_constraint(fk_name, dep_table, type_='foreignkey')

    # Drop composite PKs from junction tables
    for junction_table in JUNCTION_TABLES:
        pk_name = get_pk_constraint_name(connection, junction_table)
        if pk_name:
            op.drop_constraint(pk_name, junction_table, type_='primary')

    # Drop PKs from entity tables
    for table in ENTITY_TABLES:
        pk_name = get_pk_constraint_name(connection, table)
        if pk_name:
            op.drop_constraint(pk_name, table, type_='primary')

    # Drop indexes that will be affected (recreated in phase 5)
    # Note: Some indexes are auto-dropped with constraints, but we explicitly handle others
    indexes_to_drop = [
        ('bookmark_tags', 'ix_bookmark_tags_tag_id'),
        ('note_tags', 'ix_note_tags_tag_id'),
        ('prompt_tags', 'ix_prompt_tags_tag_id'),
        ('note_versions', 'ix_note_versions_note_id_version'),
        ('bookmarks', 'uq_bookmark_user_url_active'),
        ('prompts', 'uq_prompt_user_name_active'),
    ]
    for table, index_name in indexes_to_drop:
        existing_indexes = get_index_names(connection, table)
        if index_name in existing_indexes:
            op.drop_index(index_name, table_name=table)


def phase4_swap_columns(connection) -> None:
    """Phase 4: Drop old columns and rename new columns."""

    # Drop old FK columns from junction tables and rename new ones
    for junction_table, fk_columns in JUNCTION_TABLES.items():
        for fk_col, _ in fk_columns:
            op.drop_column(junction_table, fk_col)
            op.alter_column(junction_table, f'new_{fk_col}', new_column_name=fk_col)

    # Drop old FK columns from other dependent tables and rename new ones
    for dep_table, fk_col, _ in OTHER_FKS:
        op.drop_column(dep_table, fk_col)
        op.alter_column(dep_table, f'new_{fk_col}', new_column_name=fk_col)

    # Drop old id columns from entity tables and rename new ones
    for table in ENTITY_TABLES:
        op.drop_column(table, 'id')
        op.alter_column(table, 'new_id', new_column_name='id')


def phase5_recreate_constraints(connection) -> None:
    """Phase 5: Recreate all constraints with proper names."""

    # Recreate PKs on entity tables
    for table in ENTITY_TABLES:
        op.create_primary_key(f'{table}_pkey', table, ['id'])

    # Recreate composite PKs on junction tables
    for junction_table, fk_columns in JUNCTION_TABLES.items():
        pk_cols = [fk_col for fk_col, _ in fk_columns]
        op.create_primary_key(f'{junction_table}_pkey', junction_table, pk_cols)

    # Recreate FK constraints on junction tables
    for junction_table, fk_columns in JUNCTION_TABLES.items():
        for fk_col, ref_table in fk_columns:
            op.create_foreign_key(
                f'{junction_table}_{fk_col}_fkey',
                junction_table, ref_table,
                [fk_col], ['id'],
                ondelete='CASCADE'
            )

    # Recreate FK constraints on other dependent tables
    for dep_table, fk_col, ref_table in OTHER_FKS:
        op.create_foreign_key(
            f'{dep_table}_{fk_col}_fkey',
            dep_table, ref_table,
            [fk_col], ['id'],
            ondelete='CASCADE'
        )

    # Recreate indexes
    # Partial unique index on bookmarks (user_id + url for non-deleted)
    op.create_index(
        'uq_bookmark_user_url_active', 'bookmarks',
        ['user_id', 'url'], unique=True,
        postgresql_where=sa.text("deleted_at IS NULL")
    )

    # Partial unique index on prompts (user_id + name for non-deleted)
    op.create_index(
        'uq_prompt_user_name_active', 'prompts',
        ['user_id', 'name'], unique=True,
        postgresql_where=sa.text("deleted_at IS NULL")
    )

    # Junction table indexes for tag_id lookups
    op.create_index('ix_bookmark_tags_tag_id', 'bookmark_tags', ['tag_id'])
    op.create_index('ix_note_tags_tag_id', 'note_tags', ['tag_id'])
    op.create_index('ix_prompt_tags_tag_id', 'prompt_tags', ['tag_id'])

    # note_versions composite index
    op.create_index('ix_note_versions_note_id_version', 'note_versions', ['note_id', 'version'])


def migrate_sidebar_order_jsonb(connection) -> None:
    """Update user_settings.sidebar_order JSONB to use new content_list UUIDs."""
    list_id_mapping = ID_MAPPINGS.get('content_lists', {})

    if not list_id_mapping:
        return  # No content_lists to migrate

    rows = connection.execute(sa.text(
        "SELECT user_id, sidebar_order FROM user_settings WHERE sidebar_order IS NOT NULL"
    ))

    for row in rows:
        sidebar_order = row.sidebar_order
        if not sidebar_order or 'items' not in sidebar_order:
            continue

        def update_items(items_list: list) -> bool:
            """Recursively update list IDs in items (handles groups with nested items)."""
            changed = False
            for item in items_list:
                if item.get('type') == 'list' and isinstance(item.get('id'), int):
                    old_id = item['id']
                    if old_id in list_id_mapping:
                        item['id'] = list_id_mapping[old_id]
                        changed = True
                elif item.get('type') == 'group' and 'items' in item:
                    if update_items(item['items']):
                        changed = True
            return changed

        if update_items(sidebar_order.get('items', [])):
            connection.execute(
                sa.text("UPDATE user_settings SET sidebar_order = :new_order WHERE user_id = :user_id"),
                {"new_order": json.dumps(sidebar_order), "user_id": row.user_id}
            )


# =============================================================================
# Migration Entry Points
# =============================================================================

def upgrade() -> None:
    connection = op.get_bind()

    # Phase 1: Add new columns
    phase1_add_new_columns(connection)

    # Phase 2: Populate new columns with UUIDs and FK references
    phase2_populate_new_columns(connection)

    # Phase 3: Drop all old constraints
    phase3_drop_old_constraints(connection)

    # Phase 4: Drop old columns, rename new columns
    phase4_swap_columns(connection)

    # Phase 5: Recreate all constraints
    phase5_recreate_constraints(connection)

    # Update JSONB fields that store entity IDs
    migrate_sidebar_order_jsonb(connection)

    # Cleanup: Drop deprecated tab_order column
    op.drop_column('user_settings', 'tab_order')


def downgrade() -> None:
    raise NotImplementedError(
        "Downgrade not supported for PK type change. Restore from backup."
    )
```

**Important notes for the agent:**
- The migration uses a **phased approach** to avoid FK constraint conflicts
- All constraint names are queried from the database (no hardcoded assumptions)
- The `tags` table has a unique constraint on `(user_id, name)` - preserved since only `id` column changes
- Junction table composite PKs `(entity_id, tag_id)` are properly recreated in Phase 5
- The `content_lists` ID mapping is stored for JSONB updates to `sidebar_order`
- The `sidebar_order` JSONB field stores list IDs - migrated to UUIDs in step 5
- Test thoroughly against backup before production

### Testing Strategy

**Local testing after migration:**

After running the migration locally (`make migrate`), verify the migration worked correctly:

```bash
# 1. Verify all entity tables have UUID PKs
psql "$DATABASE_URL" -c "
  SELECT table_name, column_name, data_type
  FROM information_schema.columns
  WHERE table_name IN ('bookmarks', 'notes', 'prompts', 'content_lists', 'tags', 'api_tokens', 'note_versions')
    AND column_name = 'id'
  ORDER BY table_name;"
# All should show data_type = 'uuid'

# 2. Verify junction table FK columns are UUID type
psql "$DATABASE_URL" -c "
  SELECT table_name, column_name, data_type
  FROM information_schema.columns
  WHERE (table_name = 'bookmark_tags' AND column_name IN ('bookmark_id', 'tag_id'))
     OR (table_name = 'note_tags' AND column_name IN ('note_id', 'tag_id'))
     OR (table_name = 'prompt_tags' AND column_name IN ('prompt_id', 'tag_id'))
     OR (table_name = 'note_versions' AND column_name = 'note_id')
  ORDER BY table_name, column_name;"
# All should show data_type = 'uuid'

# 3. Verify no NULL IDs in entity tables
psql "$DATABASE_URL" -c "
  SELECT 'bookmarks' as tbl, COUNT(*) as null_ids FROM bookmarks WHERE id IS NULL
  UNION ALL SELECT 'notes', COUNT(*) FROM notes WHERE id IS NULL
  UNION ALL SELECT 'prompts', COUNT(*) FROM prompts WHERE id IS NULL
  UNION ALL SELECT 'content_lists', COUNT(*) FROM content_lists WHERE id IS NULL
  UNION ALL SELECT 'tags', COUNT(*) FROM tags WHERE id IS NULL
  UNION ALL SELECT 'api_tokens', COUNT(*) FROM api_tokens WHERE id IS NULL
  UNION ALL SELECT 'note_versions', COUNT(*) FROM note_versions WHERE id IS NULL;"
# All should return 0

# 4. Verify junction table FK integrity (both entity_id and tag_id)
psql "$DATABASE_URL" -c "
  SELECT 'bookmark_tags->bookmarks' as relation, COUNT(*) as orphaned
  FROM bookmark_tags bt LEFT JOIN bookmarks b ON bt.bookmark_id = b.id WHERE b.id IS NULL
  UNION ALL
  SELECT 'bookmark_tags->tags', COUNT(*)
  FROM bookmark_tags bt LEFT JOIN tags t ON bt.tag_id = t.id WHERE t.id IS NULL
  UNION ALL
  SELECT 'note_tags->notes', COUNT(*)
  FROM note_tags nt LEFT JOIN notes n ON nt.note_id = n.id WHERE n.id IS NULL
  UNION ALL
  SELECT 'note_tags->tags', COUNT(*)
  FROM note_tags nt LEFT JOIN tags t ON nt.tag_id = t.id WHERE t.id IS NULL
  UNION ALL
  SELECT 'prompt_tags->prompts', COUNT(*)
  FROM prompt_tags pt LEFT JOIN prompts p ON pt.prompt_id = p.id WHERE p.id IS NULL
  UNION ALL
  SELECT 'prompt_tags->tags', COUNT(*)
  FROM prompt_tags pt LEFT JOIN tags t ON pt.tag_id = t.id WHERE t.id IS NULL
  UNION ALL
  SELECT 'note_versions->notes', COUNT(*)
  FROM note_versions nv LEFT JOIN notes n ON nv.note_id = n.id WHERE n.id IS NULL;"
# All should return 0

# 5. Verify composite PKs exist on junction tables
psql "$DATABASE_URL" -c "
  SELECT tc.table_name, kcu.column_name
  FROM information_schema.table_constraints tc
  JOIN information_schema.key_column_usage kcu
    ON tc.constraint_name = kcu.constraint_name
  WHERE tc.constraint_type = 'PRIMARY KEY'
    AND tc.table_name IN ('bookmark_tags', 'note_tags', 'prompt_tags')
  ORDER BY tc.table_name, kcu.ordinal_position;"
# Should show composite PKs: bookmark_tags(bookmark_id, tag_id), etc.

# 6. Verify indexes exist
psql "$DATABASE_URL" -c "
  SELECT indexname FROM pg_indexes
  WHERE tablename IN ('bookmarks', 'notes', 'prompts', 'bookmark_tags', 'note_tags', 'prompt_tags', 'note_versions')
  ORDER BY tablename, indexname;"
# Should include: uq_bookmark_user_url_active, uq_prompt_user_name_active,
#                 ix_bookmark_tags_tag_id, ix_note_tags_tag_id,
#                 ix_prompt_tags_tag_id, ix_note_versions_note_id_version

# 7. Verify sidebar_order JSONB has UUID strings (not integers) for list IDs
psql "$DATABASE_URL" -c "
  SELECT user_id, sidebar_order
  FROM user_settings
  WHERE sidebar_order IS NOT NULL
  LIMIT 5;"
# List IDs should be UUID strings like '01938a12-3b45-7c67-8d90-ef1234567890', not integers

# 8. Verify tab_order column is dropped
psql "$DATABASE_URL" -c "
  SELECT column_name FROM information_schema.columns
  WHERE table_name = 'user_settings' AND column_name = 'tab_order';"
# Should return 0 rows
```

**Note:** Full test suite (`make tests`) runs in Milestone 3 after code is updated. Production backup testing is in Milestone 6.

### Dependencies
Milestone 1 (UUIDv7Mixin must exist)

### Risk Factors
- **Backup required**: Always backup before running on production
- **No downgrade**: Rollback means restoring from backup

---

## Milestone 3: Update Backend Code

### Goal
Update all models, schemas, routers, and services to use UUID strings instead of integers.

### Success Criteria
- All 6 models use `UUIDv7Mixin`
- All schemas have `id: str`
- All routers have `str` path parameters
- All services have updated type hints
- All existing tests pass (with updates for string IDs)

### Key Changes

**1. Update models to use UUIDv7Mixin:**

Remove `id: Mapped[int] = mapped_column(primary_key=True)` from each model and add `UUIDv7Mixin`:

```python
# backend/src/models/bookmark.py
from models.base import ArchivableMixin, Base, TimestampMixin, UUIDv7Mixin

class Bookmark(Base, TimestampMixin, ArchivableMixin, UUIDv7Mixin):
    __tablename__ = "bookmarks"
    # Remove: id: Mapped[int] = mapped_column(primary_key=True)
    # ... rest unchanged
```

Apply same pattern to:
- `backend/src/models/note.py`
- `backend/src/models/prompt.py`
- `backend/src/models/content_list.py`
- `backend/src/models/tag.py`
- `backend/src/models/api_token.py`
- `backend/src/models/note_version.py` (also update `note_id` type to `UUID`)

**Cleanup deprecated column:**

Remove `tab_order` from `backend/src/models/user_settings.py` (column dropped in migration):

```python
# Remove this entire field definition:
# tab_order: Mapped[dict | None] = mapped_column(
#     JSONB,
#     nullable=True,
#     comment="DEPRECATED - Use sidebar_order instead",
# )
```

**Important timing note:** Remove `tab_order` from the model BEFORE running the migration. Since no code uses `tab_order`, removing it from the model won't break anything. The migration will then drop the column from the database, and the model will match the DB schema.

**2. Update junction table definitions in `backend/src/models/tag.py`:**

```python
from sqlalchemy.dialects.postgresql import UUID as PG_UUID

bookmark_tags = Table(
    "bookmark_tags",
    Base.metadata,
    Column(
        "bookmark_id",
        PG_UUID(as_uuid=True),  # Changed from Integer
        ForeignKey("bookmarks.id", ondelete="CASCADE"),
        primary_key=True,
    ),
    Column(
        "tag_id",
        PG_UUID(as_uuid=True),  # Changed from Integer
        ForeignKey("tags.id", ondelete="CASCADE"),
        primary_key=True,
    ),
    Index("ix_bookmark_tags_tag_id", "tag_id"),
)
# Same for note_tags and prompt_tags
```

**3. Update schemas - change `id: int` to `id: UUID`:**

Using `UUID` type in schemas provides:
- Type safety (consistent with routers/services)
- Automatic serialization to string in JSON responses
- Validation on input (malformed UUIDs rejected)

Files to update:
- `backend/src/schemas/bookmark.py`: `BookmarkListItem`, `BookmarkResponse`
- `backend/src/schemas/note.py`: `NoteListItem`, `NoteResponse`
- `backend/src/schemas/prompt.py`: `PromptListItem`, `PromptResponse`
- `backend/src/schemas/content.py`: `ContentListItem` (unified content schema)
- `backend/src/schemas/content_list.py`: `ContentListResponse`
- `backend/src/schemas/tag.py`: `TagResponse`
- `backend/src/schemas/token.py`: `TokenResponse`, `TokenCreateResponse`
- `backend/src/schemas/sidebar.py`: `SidebarListItem.id`, `SidebarListItemComputed.id`

Example:
```python
from uuid import UUID

class BookmarkListItem(BaseModel):
    model_config = ConfigDict(from_attributes=True)

    id: UUID  # Changed from int - Pydantic auto-serializes to string in JSON
    # ... rest unchanged
```

**4. Update routers - use UUID type for path parameters:**

Using `UUID` type directly provides automatic 422 validation on malformed UUIDs (instead of letting invalid strings propagate to the database):

```python
from uuid import UUID

@router.get("/{bookmark_id}")
async def get_bookmark(bookmark_id: UUID) -> BookmarkResponse:
    # FastAPI automatically returns 422 if bookmark_id is not a valid UUID
    ...
```

Files to update:
- `backend/src/api/routers/bookmarks.py`: `bookmark_id: int` → `bookmark_id: UUID`
- `backend/src/api/routers/notes.py`: `note_id: int` → `note_id: UUID`
- `backend/src/api/routers/prompts.py`: `prompt_id: int` → `prompt_id: UUID`
- `backend/src/api/routers/lists.py`: `list_id: int` → `list_id: UUID`
- `backend/src/api/routers/tags.py`: `tag_id: int` → `tag_id: UUID`
- `backend/src/api/routers/tokens.py`: `token_id: int` → `token_id: UUID`
- `backend/src/api/routers/content.py`: `list_id: int | None` → `UUID | None`

Also update `list_id` query parameters in bookmarks, notes, prompts, content routers to `UUID | None`.

**5. Update services - change type hints:**

Files to update:
- `backend/src/services/bookmark_service.py` (also `DuplicateUrlError.existing_bookmark_id`)
- `backend/src/services/note_service.py`
- `backend/src/services/prompt_service.py`
- `backend/src/services/list_service.py`
- `backend/src/services/tag_service.py`
- `backend/src/services/token_service.py`
- `backend/src/services/content_list_service.py`: `list_id: int` → `UUID`
- `backend/src/services/content_service.py`: `bookmark_ids: list[int]`, `note_ids: list[int]`, `prompt_ids: list[int]` → `list[UUID]`
- `backend/src/services/sidebar_service.py`: `list_id: int` → `UUID`, `set[int]` → `set[UUID]`
- `backend/src/services/exceptions.py`: `ListNotFoundError.__init__(list_id: int)` → `UUID`

Example:
```python
from uuid import UUID

async def get_bookmark(
    session: AsyncSession,
    user_id: int,  # user_id stays int (users table not migrated)
    bookmark_id: UUID,  # Changed from int
    ...
) -> Bookmark | None:
```

**6. Update TaggableEntity Protocol in `backend/src/services/base_entity_service.py`:**

```python
from uuid import UUID

class TaggableEntity(Protocol):
    """Protocol defining the interface for entities that support tagging and soft-delete."""

    id: UUID  # Changed from int
    user_id: int  # Stays int (users table not migrated)
    # ... rest unchanged
```

Also update `entity_id: int` parameters in `BaseEntityService` methods to `entity_id: UUID`.

### Testing Strategy

**Update existing tests:**

```bash
# Find all tests that need updates
grep -rn "\.id == [0-9]" backend/tests/
grep -rn '"id": [0-9]' backend/tests/
grep -rn "_id: int" backend/tests/
grep -rn "bookmark_id=" backend/tests/
grep -rn "note_id=" backend/tests/
grep -rn "prompt_id=" backend/tests/
```

Common patterns to update:
- `assert response.json()["id"] == 1` → `assert isinstance(response.json()["id"], str)` and validate UUID format
- `bookmark_id: int` → `bookmark_id: UUID`
- Integer comparisons → UUID comparisons

**New tests to add:**
- Test 422 returned for malformed UUID path parameters (e.g., `/bookmarks/not-a-uuid`)
- Test 404 returned for valid UUID that doesn't exist

**Run full test suite:**
```bash
make tests  # Runs linting + all backend + frontend tests
```

### Dependencies
Milestone 2 (database must be migrated)

### Risk Factors
- Many files to update - use grep to find all occurrences
- Test fixtures may create entities with hardcoded integer IDs

---

## Milestone 4: Update MCP Servers

### Goal
Update MCP servers to use UUID strings for ID parameters.

### Success Criteria
- MCP tools accept `id` parameters as `str`
- MCP server tests pass

### Key Changes

**1. Update `backend/src/mcp_server/server.py`:**

```python
# get_bookmark
bookmark_id: Annotated[str, Field(description="The UUID of the bookmark to retrieve")]

# get_note
note_id: Annotated[str, Field(description="The UUID of the note to retrieve")]
```

**2. Update `backend/src/prompt_mcp_server/server.py`:**

```python
# get_prompt tool - update prompt_id parameter
prompt_id: Annotated[str, Field(description="The UUID of the prompt to retrieve")]
```

### Testing Strategy
- Test `get_bookmark` and `get_note` work with UUID string IDs
- Test 404 returned for invalid/nonexistent UUID
- Run MCP server tests

### Dependencies
Milestone 3 (backend code must be updated)

### Risk Factors
Very low risk - minimal code changes

---

## Milestone 5: Update Frontend

### Goal
Update frontend TypeScript types to expect `id` as string.

### Success Criteria
- TypeScript types use `id: string` instead of `id: number`
- All existing code using `bookmark.id`, `note.id`, etc. continues working
- URL routes continue working
- All frontend tests pass
- TypeScript compiles without errors

### Key Changes

**1. Update `frontend/src/types.ts`:**

```typescript
export interface BookmarkListItem {
  id: string  // Changed from number
  // ... rest unchanged
}

export interface NoteListItem {
  id: string  // Changed from number
  // ...
}

export interface PromptListItem {
  id: string  // Changed from number
  // ...
}

export interface ContentList {
  id: string  // Changed from number
  // ...
}

export interface Tag {
  id: string  // Changed from number
  // ...
}

export interface Token {
  id: string  // Changed from number
  // ...
}

export interface ContentListItem {
  id: string  // Changed from number
  // ...
}

export interface SidebarListItem {
  type: 'list'
  id: string  // Changed from number
}

// Update query param types
export interface NoteSearchParams {
  // ...
  list_id?: string  // Changed from number
}

export interface BookmarkSearchParams {
  // ...
  list_id?: string  // Changed from number
}

export interface PromptSearchParams {
  // ...
  list_id?: string  // Changed from number
}

export interface ContentSearchParams {
  // ...
  list_id?: string  // Changed from number
}
```

**2. Search for any numeric operations on IDs:**

```bash
cd frontend
grep -rn "\.id ===" src/
grep -rn "\.id ==" src/
grep -rn "\.id <" src/
grep -rn "\.id >" src/
grep -rn "parseInt.*id" src/
grep -rn "Number.*id" src/
```

Fix any code that treats IDs as numbers.

### Testing Strategy

```bash
cd frontend

# TypeScript compilation
npm run build

# Run tests
npm run test:run

# Linting
npm run lint
```

Manual testing:
- Create, read, update, delete bookmarks/notes/prompts
- Navigate via URLs
- Test list filtering with `list_id`

### Dependencies
All backend milestones complete

### Risk Factors
- Any code comparing IDs as numbers would break
- Any code parsing IDs as integers would break

---

## Milestone 6: Pre-Production Verification

### Goal
Test the migration against a copy of production data before deploying. This is the final gate before production.

### Success Criteria
- Migration runs successfully on production backup
- All verification checks pass
- No data loss or corruption

### Prerequisites
- All previous milestones complete
- All tests passing (`make tests`)
- Frontend builds and tests pass

### Steps

**1. Clone production database to backup:**

```bash
# Set environment variables
export PROD_DATABASE_URL="postgresql://postgres:..."
export BACKUP_DATABASE_URL="postgresql://postgres:..."

# Copy production database
PGSSLMODE=require pg_dump --dbname="$PROD_DATABASE_URL" -F c \
  | PGSSLMODE=require pg_restore --clean --if-exists --no-owner --no-privileges \
    --dbname="$BACKUP_DATABASE_URL"
```

**2. Record pre-migration row counts:**

```bash
PGSSLMODE=require psql "$BACKUP_DATABASE_URL" -c "
  SELECT 'bookmarks' as tbl, COUNT(*) FROM bookmarks
  UNION ALL SELECT 'notes', COUNT(*) FROM notes
  UNION ALL SELECT 'prompts', COUNT(*) FROM prompts
  UNION ALL SELECT 'content_lists', COUNT(*) FROM content_lists
  UNION ALL SELECT 'tags', COUNT(*) FROM tags
  UNION ALL SELECT 'api_tokens', COUNT(*) FROM api_tokens
  UNION ALL SELECT 'note_versions', COUNT(*) FROM note_versions
  UNION ALL SELECT 'bookmark_tags', COUNT(*) FROM bookmark_tags
  UNION ALL SELECT 'note_tags', COUNT(*) FROM note_tags
  UNION ALL SELECT 'prompt_tags', COUNT(*) FROM prompt_tags;"
```

**3. Verify data integrity (no orphaned records):**

```bash
# Check for orphaned junction table records BEFORE migration
# If any count > 0, investigate and fix before proceeding
PGSSLMODE=require psql "$BACKUP_DATABASE_URL" -c "
  SELECT
    (SELECT COUNT(*) FROM bookmark_tags bt
     LEFT JOIN bookmarks b ON bt.bookmark_id = b.id WHERE b.id IS NULL) as orphaned_bookmark_tags,
    (SELECT COUNT(*) FROM bookmark_tags bt
     LEFT JOIN tags t ON bt.tag_id = t.id WHERE t.id IS NULL) as orphaned_bookmark_tag_refs,
    (SELECT COUNT(*) FROM note_tags nt
     LEFT JOIN notes n ON nt.note_id = n.id WHERE n.id IS NULL) as orphaned_note_tags,
    (SELECT COUNT(*) FROM note_tags nt
     LEFT JOIN tags t ON nt.tag_id = t.id WHERE t.id IS NULL) as orphaned_note_tag_refs,
    (SELECT COUNT(*) FROM prompt_tags pt
     LEFT JOIN prompts p ON pt.prompt_id = p.id WHERE p.id IS NULL) as orphaned_prompt_tags,
    (SELECT COUNT(*) FROM prompt_tags pt
     LEFT JOIN tags t ON pt.tag_id = t.id WHERE t.id IS NULL) as orphaned_prompt_tag_refs,
    (SELECT COUNT(*) FROM note_versions nv
     LEFT JOIN notes n ON nv.note_id = n.id WHERE n.id IS NULL) as orphaned_note_versions;"
# ALL columns should be 0. If not, DO NOT proceed - fix orphaned data first.
```

**4. Run migration:**

```bash
DATABASE_URL="$BACKUP_DATABASE_URL" uv run alembic upgrade head
```

**5. Run verification checks:**

```bash
# 1. Verify all entity tables have UUID PKs
PGSSLMODE=require psql "$BACKUP_DATABASE_URL" -c "
  SELECT table_name, column_name, data_type
  FROM information_schema.columns
  WHERE table_name IN ('bookmarks', 'notes', 'prompts', 'content_lists', 'tags', 'api_tokens', 'note_versions')
    AND column_name = 'id'
  ORDER BY table_name;"
# All should show data_type = 'uuid'

# 2. Verify junction table FK columns are UUID type
PGSSLMODE=require psql "$BACKUP_DATABASE_URL" -c "
  SELECT table_name, column_name, data_type
  FROM information_schema.columns
  WHERE (table_name = 'bookmark_tags' AND column_name IN ('bookmark_id', 'tag_id'))
     OR (table_name = 'note_tags' AND column_name IN ('note_id', 'tag_id'))
     OR (table_name = 'prompt_tags' AND column_name IN ('prompt_id', 'tag_id'))
     OR (table_name = 'note_versions' AND column_name = 'note_id')
  ORDER BY table_name, column_name;"
# All should show data_type = 'uuid'

# 3. Verify no NULL IDs in entity tables
PGSSLMODE=require psql "$BACKUP_DATABASE_URL" -c "
  SELECT 'bookmarks' as tbl, COUNT(*) as null_ids FROM bookmarks WHERE id IS NULL
  UNION ALL SELECT 'notes', COUNT(*) FROM notes WHERE id IS NULL
  UNION ALL SELECT 'prompts', COUNT(*) FROM prompts WHERE id IS NULL
  UNION ALL SELECT 'content_lists', COUNT(*) FROM content_lists WHERE id IS NULL
  UNION ALL SELECT 'tags', COUNT(*) FROM tags WHERE id IS NULL
  UNION ALL SELECT 'api_tokens', COUNT(*) FROM api_tokens WHERE id IS NULL
  UNION ALL SELECT 'note_versions', COUNT(*) FROM note_versions WHERE id IS NULL;"
# All should return 0

# 4. Verify junction table FK integrity (both entity_id and tag_id)
PGSSLMODE=require psql "$BACKUP_DATABASE_URL" -c "
  SELECT 'bookmark_tags->bookmarks' as relation, COUNT(*) as orphaned
  FROM bookmark_tags bt LEFT JOIN bookmarks b ON bt.bookmark_id = b.id WHERE b.id IS NULL
  UNION ALL
  SELECT 'bookmark_tags->tags', COUNT(*)
  FROM bookmark_tags bt LEFT JOIN tags t ON bt.tag_id = t.id WHERE t.id IS NULL
  UNION ALL
  SELECT 'note_tags->notes', COUNT(*)
  FROM note_tags nt LEFT JOIN notes n ON nt.note_id = n.id WHERE n.id IS NULL
  UNION ALL
  SELECT 'note_tags->tags', COUNT(*)
  FROM note_tags nt LEFT JOIN tags t ON nt.tag_id = t.id WHERE t.id IS NULL
  UNION ALL
  SELECT 'prompt_tags->prompts', COUNT(*)
  FROM prompt_tags pt LEFT JOIN prompts p ON pt.prompt_id = p.id WHERE p.id IS NULL
  UNION ALL
  SELECT 'prompt_tags->tags', COUNT(*)
  FROM prompt_tags pt LEFT JOIN tags t ON pt.tag_id = t.id WHERE t.id IS NULL
  UNION ALL
  SELECT 'note_versions->notes', COUNT(*)
  FROM note_versions nv LEFT JOIN notes n ON nv.note_id = n.id WHERE n.id IS NULL;"
# All should return 0

# 5. Verify composite PKs exist on junction tables
PGSSLMODE=require psql "$BACKUP_DATABASE_URL" -c "
  SELECT tc.table_name, kcu.column_name
  FROM information_schema.table_constraints tc
  JOIN information_schema.key_column_usage kcu
    ON tc.constraint_name = kcu.constraint_name
  WHERE tc.constraint_type = 'PRIMARY KEY'
    AND tc.table_name IN ('bookmark_tags', 'note_tags', 'prompt_tags')
  ORDER BY tc.table_name, kcu.ordinal_position;"
# Should show composite PKs: bookmark_tags(bookmark_id, tag_id), etc.

# 6. Verify sidebar_order JSONB has UUID strings for list IDs
PGSSLMODE=require psql "$BACKUP_DATABASE_URL" -c "
  SELECT user_id, sidebar_order
  FROM user_settings
  WHERE sidebar_order IS NOT NULL
  LIMIT 5;"
# List IDs should be UUID strings, not integers

# 7. Verify tab_order column is dropped
PGSSLMODE=require psql "$BACKUP_DATABASE_URL" -c "
  SELECT column_name FROM information_schema.columns
  WHERE table_name = 'user_settings' AND column_name = 'tab_order';"
# Should return 0 rows

# 8. Verify row counts match pre-migration (re-run step 2 query and compare)
```

**6. Test API against backup (optional but recommended):**

```bash
# Point local API at backup database
DATABASE_URL="$BACKUP_DATABASE_URL" make run

# In another terminal, run a quick smoke test
curl http://localhost:8000/health
# Test a few endpoints manually to verify data looks correct
```

### Dependencies
Milestones 1-5 complete

### Risk Factors
- Ensure backup database is isolated (not accidentally pointing at production)
- Time the migration to estimate production downtime

---

## Summary

| Milestone | Component | Scope |
|-----------|-----------|-------|
| 1 | Foundation | `uuid7` library + `UUIDv7Mixin` |
| 2 | Database Migration | Single migration for all 7 tables + 3 junction tables |
| 3 | Backend Code | Models, schemas, routers, services |
| 4 | MCP Servers | ID parameter types |
| 5 | Frontend | TypeScript types |
| 6 | Pre-Production Verification | Test migration on production backup |

Total: 6 milestones. Each milestone should be reviewed before proceeding to the next.

### Migration Workflow

1. **Milestone 1**: Add library and mixin (no DB changes)
2. **Milestone 2**: Write migration, run locally, verify with SQL checks
3. **Milestone 3**: Update backend code, run `make tests`
4. **Milestone 4**: Update MCP servers
5. **Milestone 5**: Update frontend, run `npm run build && npm run test:run`
6. **Milestone 6**: Clone production DB → run migration → verify → smoke test API
7. **Deploy**: Run migration on production, deploy new code

### Important Notes

- **No backwards compatibility**: Old integer IDs will not work after migration
- **API breaking change**: Clients must update to expect string IDs (UUID format)
- **Database backup required**: Test migration on backup before production
- **Downgrade not supported**: Rollback means restoring from pre-migration backup
- **Atomic migration**: All tables converted in single transaction
- **JSONB migration**: `sidebar_order` field updated to use UUID strings for list IDs
- **Cleanup**: Deprecated `tab_order` column dropped from `user_settings` (replaced by `sidebar_order`)
